#查看显卡支持算力（sm）：https://developer.nvidia.com/cuda-gpus ✔✔✔
#查看CUDA库支持的算力（sm）：nvcc --list-gpu-arch ✔✔✔
#参考：https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html
nvidia-smi -L  查看GPU类型

#pytorch,torchvision,torchaudio以及它们之间的版本对应关系
#参考：#https://blog.csdn.net/toopoo/article/details/124825357

1、安装通用依赖
    yum update
    dnf groupinstall -y "Development Tools"
    yum install -y gcc g++ cmake make python-pip python3-devel ninja-build.aarch64 numactl-devel.aarch64 wget git
    dnf install -y kernel-devel kernel-headers  tar zip pciutils

2、安装conda
   Linux 用户，可以下载 Anaconda3-2022.05-Linux-aarch64.sh
   wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-aarch64.sh
   # 先进行 conda 的安装:
   #yum install gtk3-devel 
   #apt-get install libgtk-3-dev
   bash Anaconda3-2022.05-Linux-aarch64.sh
   source ~/.bashrc
   conda --version

   #让 conda shell 环境常驻(不常使用不推荐):
   eval "$(~/anaconda3/bin/conda shell.bash hook)"

   #国内用户，推荐在使用 Conda 时，先进行软件源配置操作：
   vi ~/.condarc
   '''
channels:
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
 - defaults
show_channel_urls: true
   '''
   完成了 ~/.condarc 的内容修改之后，先重启 Shell，接着使用 conda info 就可以检查软件源是否配置成功了
   conda info

   #手动激活 conda shell
   conda create -n torch python=3.11 -y
   
   #激活和退出专用环境：
   conda activate torch
   conda deactivate
   conda list                  当前env基础环境
   conda list -n torch
   conda env list              env列表
   conda activate base
   conda env remove -n torch   删除env
   conda remove -n torch --all 
   conda install --use-local xxxx.tar.bz2(xxxx.tar.bz2是包的绝对路径) 安装离线py包
   
   #当完成环境激活之后，我们同样先来完成pip软件源的切换
   pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

3、安装安装 NVIDIA 驱动(驱动版本可以大于CUDA版本)
   # 禁用nouveau驱动
    echo "blacklist nouveau" | sudo tee /etc/modprobe.d/blacklist-nouveau.conf
    sudo dracut --force

    https://www.nvidia.com/en-us/drivers/
    wget https://us.download.nvidia.com/tesla/560.35.03/NVIDIA-Linux-aarch64-560.35.03.run
    sh NVIDIA-Linux-aarch64-560.35.03.run --kernel-source-path /usr/src/kernels/5.10.0-136.108.0.188.oe2203sp1.aarch64

    # 验证驱动
    nvidia-smi

    #驱动卸载 https://cloud.tencent.com/document/product/560/111921
    nvidia-installer --uninstall -s
    lsmod | grep nvidia 检查驱动是否卸载干净
    fuser -k /dev/nvidia*; rmmod nvidia_modeset; rmmod nvidia_drm; rmmod nvidia_uvm; rmmod nvidia;

4、安装 CUDA 12.6
    wget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.28.03_linux_sbsa.run
    sh cuda_12.6.0_560.28.03_linux_sbsa.run --tmpdir=/usr/tmp2

    echo查看$PATH和$LD_LIBRARY_PATH和$CUDA_HOME变量

    #有些会有ln链接
    lrwxrwxrwx  1 root root   21 Mar 26 08:44 cuda -> /usr/local/cuda-12.6/
    drwxr-xr-x 14 root root 4096 Mar 26 08:46 cuda-12.6

    # 常规添加环境变量
    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
    echo 'export CUDA_HOME=/usr/local/cuda:$CUDA_HOME' >> ~/.bashrc
    source ~/.bashrc

    #部分容器环境需要使用下面：
    echo 'export PATH=/usr/local/cuda/bin' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64' >> ~/.bashrc
    echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
    source ~/.bashrc

    #部分VM环境需要使用下面：✔✔✔
    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64' >> ~/.bashrc
    echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
    source ~/.bashrc

    安装完后再次确认 nvidia-smi 可用

5、安装依赖： 
               cmake源码安装（注意版本> 3.26.0）：
                   wget https://cmake.org/files/v3.26/cmake-3.26.6-linux-aarch64.sh
                   sh cmake-3.26.6-linux-aarch64.sh
                   mv cmake-3.26.6-linux-aarch64 /opt/cmake-3.26.6
                   ln -sf /opt/cmake-3.26.6/bin/*  /usr/bin
                   其他版本：
                   wget https://github.com/Kitware/CMake/releases/download/v3.31.2/cmake-3.31.2-linux-aarch64.sh
              torch depends：pip --default-timeout=1000  install numpy -i https://mirrors.huaweicloud.com/repository/pypi/simple
              vllm depends： pip --default-timeout=1000 install setuptools_scm  -i https://mirrors.huaweicloud.com/repository/pypi/simple
                             yum install kmod


6、安装torch-2.6.0（supports CUDA capabilities sm_50 sm_80 sm_86 sm_89 sm_90 sm_90a，不适配T4卡 sm_75，查看GPU的 compute capability）
    #查看显卡支持算力：https://developer.nvidia.com/cuda-gpus
    #版本选择：https://download.pytorch.org/whl/cu126/torch/
               https://download.pytorch.org/whl/nightly/cu126
    选择合适的cuda版本和python版本
    wget https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-linux_aarch64.whl#sha256=d4809b188f5c9b9753f7578085b79ae1f5d9c36a3fffc122e83e446ecf251325 
    pip --default-timeout=1000 install torch-2.6.0+cu126-cp311-cp311-linux_aarch64.whl -i https://mirrors.huaweicloud.com/repository/pypi/simple
    pip  uninstall torch-2.6.0+cu126-cp311-cp311-linux_aarch64.whl

    # 验证GPU支持
    python -c "import torch; print(torch.cuda.device_count())"
    python -c "import torch; print(torch.__version__); print(torch.__path__)"
    python -c "import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0)); print(torch.cuda.get_arch_list())"
    '''
    import torch
    print(torch.__version__)
    print(torch.cuda.is_available())
    print(torch.cuda.get_device_name(0))
    print(torch.cuda.get_arch_list())  # 应包含 sm_75
    '''

    1、UserWarning: Can't initialize NVML
      warnings.warn("Can't initialize NVML")
    重新安装nvidia驱动

    pip list
    pip uninstall torch

    2、Tesla T4 with CUDA capability sm_75 is not compatible with the current PyTorch installation.
    The current PyTorch install supports CUDA capabilities sm_50 sm_80 sm_86 sm_89 sm_90 sm_90a.
    #https://github.com/pytorch/pytorch/issues/16733 重新从源码编译 ✔✔✔
    #https://blog.csdn.net/air__Heaven/article/details/134848423 编译参考
    1) 安装驱动和cuda (cuda还用12.6)
    2）安装magma加速 https://anaconda.org/pytorch/repo
        #conda install mkl mkl-include 没用 CPU 数学加速（Intel 优化）
        #pip install mkl-static mkl-include 没用 CPU 数学加速（Intel 优化）
        wget https://anaconda.org/pytorch/magma-cuda126/2.6.1/download/linux-64/magma-cuda126-2.6.1-1.tar.bz2 （x86不能用）
        conda install  --use-local  magma-cuda126-2.6.1-1.tar.bz2（x86不能用）
            conda list|grep magma-cuda126
            conda uninstall magma-cuda126

        git clone https://github.com/icl-utk-edu/magma.git ✔✔✔ 当torch需要blas加速才需要编译
        yum install -y  openblas-devel blas-devel lapack-devel
        ls /usr/lib64/libopenblas.*
        -----------------------------------
        cd magma
        cp make.inc-examples/make.inc.openblas make.inc
        vi make.inc
        '''
        OPENBLASDIR ?= /usr
        GPU_TARGET = sm_50 sm_60 sm_70 sm_75 sm_80 sm_89 sm_90  指定支持的算力 ,还要查看nvcc --list-gpu-arch
        '''
        mkdir /usr/local/magma
        make -j4 prefix=/usr/local/magma
        make install

        -DGPU_TARGET='target', where target includes one or more of:
            Kepler, Maxwell, Pascal, Volta, Turing, Ampere
            or valid sm_[0-9][0-9] for NVIDIA GPUs.
             sm_30 sm_35 sm_50 sm_60 sm_70 sm_75 sm_80 sm_89 sm_90 sm_90a

        检查架构：ar -t libmagma.a
                  ar -x libmagma.a magma_generated_cpotf2_kernels_var.cu.o
                  file magma_generated_cpotf2_kernels_var.cu.o
    2) 安装 cuDNN #https://blog.csdn.net/YY007H/article/details/134772564
       下载页面 #https://developer.nvidia.com/rdp/cudnn-archive
        # 解压 cuDNN 包（以 cuDNN 8.9.7 为例）网页下载
        https://developer.nvidia.com/downloads/compute/cudnn/secure/8.9.7/local_installers/12.x/cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz/
        tar -xvf cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz

        # 复制文件到 CUDA 目录
           #有些会有ln链接
           lrwxrwxrwx  1 root root   21 Mar 26 08:44 cuda -> /usr/local/cuda-12.6/
           drwxr-xr-x 14 root root 4096 Mar 26 08:46 cuda-12.6
        cp -r cudnn-*-archive/include/* /usr/local/cuda/include/
        cp -r cudnn-*-archive/lib/* /usr/local/cuda/lib64/

        # 更新动态链接库缓存
        ldconfig
        #检查版本
        cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2

    2) 安装nccl #https://blog.csdn.net/Scenery0519/article/details/128081062
        下载页面 https://developer.nvidia.com/nccl/nccl-legacy-downloads
           tar -xvf nccl_2.24.3-1+cuda12.6_aarch64.txz
           cp -r nccl_*/lib/* /usr/local/cuda/lib64/
           cp nccl_*/include/* /usr/local/cuda/include/

    3) conda install cmake 或者cmake源码安装（注意版本> 3.26.0）
    3) conda install ninja  #默认使用，python setup.py install --cmake-only 关闭

    4) git clone --recursive https://github.com/pytorch/pytorch
       cd pytorch
       git checkout v2.6.0
       #递归下载其中的链接包
       git submodule sync
       git submodule update --init --recursive

       export USE_PRIORITIZED_TEXT_FOR_LD=1
       export USE_BLAS=1
       export BLAS=OpenBLAS
       export USE_LAPACK=1
       export USE_MAGMA=1
       export USE_DISTRIBUTED=1
       export USE_CUDA=1
       export USE_NCCL=1
       export USE_SYSTEM_NCCL=ON
       export MAX_JOBS=4
       export CUDA_PATH=${CUDA_HOME}
       export CUDA_BIN_PATH=${CUDA_PATH}/bin
       export CMAKE_CUDA_COMPILER=${CUDA_BIN_PATH}/nvcc
       export CUDNN_LIBRARY_PATH=${CUDA_PATH}/lib64
       export CUDNN_INCLUDE_PATH=${CUDA_PATH}/include
       export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
       export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9;9.0"
       export CMAKE_CUDA_ARCHITECTURES="75;80;86;89;90"
       pip  --default-timeout=1000 install -r requirements.txt -i https://mirrors.huaweicloud.com/repository/pypi/simple
       ln -sf /usr/lib64/libstdc++.so.6 /root/anaconda3/envs/vllm/lib/libstdc++.so.6
       # 清理安装
       python setup.py clean
       # 直接安装
       python setup.py install
       python setup.py install --cmake 重新编译
       # 或者 编译成 whl安装文件，编译成功后在dist文件下面，可通过 pip install torch-xxxx.whl 安装
       python setup.py bdist_wheel
       python setup.py bdist_wheel --cmake 重新编译
       # 后台打印
       nohup python setup.py bdist_wheel > build.log 2>&1 &
       nohup python setup.py bdist_wheel --cmake > build.log 2>&1 &     重新编译

       安装包目录 pytorch/dist/

       conda环境下才会出现的问题：
       报错1：/root/anaconda3/envs/vllm/lib/libstdc++.so.6: error: version lookup error: version `GLIBCXX_3.4.30' not found (required by /home/pytorch/build/lib/libtorch_cpu.so) (continued
             排查：strings /root/anaconda3/envs/vllm/lib/libstdc++.so.6 | grep GLIBCXX
                   strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
             解决：ln -sf /usr/lib64/libstdc++.so.6 /root/anaconda3/envs/vllm/lib/libstdc++.so.6 ✔✔✔
       报错2：/root/anaconda3/envs/vllm/compiler_compat/ld: cannot find LINKER_VERSION: No such file or directory
             排查：cat /home/pytorch/cmake/linker_script.ld|grep  LINKER_VERSION
             解决:修改 linker_script.ld 文件  .comment 0 (INFO) : { *(.comment); }   怀疑conda环境的问题，链接器脚本与当前工具链不兼容的问题 ld --version
             方法二：不使用conda环境，直接使用主机gcc，未尝试

       定义在setup.py里：
       export CMAKE_CUDA_ARCHITECTURES=native #自动检测本地 GPU 架构：CMake 会通过检测当前机器的 GPU 型号，自动选择对应的计算能力（如 sm_86）。仅编译本地 GPU 支持的代码：生成的二进制文件仅包含当前机器 GPU 架构的代码，无法在其他架构 GPU 上高效运行。
       export USE_QNNPACK=1    # 建议启用（ARM cpu量化优化）
       export USE_XNNPACK=1    # 建议启用（ARM cpu通用优化）
       export USE_CUDA=1：用于设置一个名为 USE_CUDA 的环境变量，并将其值设置为 1。可能用于告知后续的脚本或程序，在构建过程中需要使用 CUDA 进行加速或其他相关操作。
       export MAX_JOBS=4：（4~推荐32G内存）设置一个名为 MAX_JOBS 的环境变量，并将其值设置为 4，可能用于指定并行编译任务的最大数量。这个按机器进行设置，太大了容易崩溃。
       export USE_SYSTEM_NCCL=ON：设置一个环境变量 USE_SYSTEM_NCCL 的值为 ON，表明使用系统中已安装的 NCCL 库。
       export USE_BLAS=1
       export BLAS=OpenBLAS
       export USE_LAPACK=1
       export USE_MAGMA=1 #OpenBLAS加速
       export USE_DISTRIBUTED=1 :PyTorch 会启用分布式训练支持
          export USE_DISTRIBUTED=1    # 启用分布式支持（必须）
          export USE_MPI=1           # 启用 OpenMPI 后端 yum install openmpi openmpi-devel
          export USE_CUDA=1          # 若需 GPU 支持
          export USE_NCCL=1
          export USE_GLOO=0          # 可选：禁用 Gloo 后端（若不需要）
          OpenMPI 后端需通过 USE_MPI=1 单独启用，并依赖 OpenMPI 的安装。
          根据需求灵活配置后端（Gloo/NCCL/MPI），避免冗余功能。

       pip wheel . (打包???)

'''
--
-- ******** Summary ********
-- General:
--   CMake version         : 3.31.6
--   CMake command         : /root/anaconda3/envs/vllm/lib/python3.11/site-packages/cmake/data/bin/cmake
--   System                : Linux
--   C++ compiler          : /usr/bin/c++
--   C++ compiler id       : GNU
--   C++ compiler version  : 12.3.1
--   Using ccache if found : ON
--   Found ccache          : CCACHE_PROGRAM-NOTFOUND
--   CXX flags             : -ffunction-sections -fdata-sections -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_PYTORCH_QNNPACK -DAT_BUILD_ARM_VEC256_WITH_SLEEF -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow
--   Shared LD flags       : -T/home/pytorch/cmake/linker_script.ld -Wl,--no-as-needed -rdynamic
--   Static LD flags       :
--   Module LD flags       : -T/home/pytorch/cmake/linker_script.ld
--   Build type            : Release
--   Compile definitions   : CAFFE2_PERF_WITH_SVE=1;ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;ONNX_NAMESPACE=onnx_torch;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1;USE_EXTERNAL_MZCRC;MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS;FLASHATTENTION_DISABLE_ALIBI;AT_BUILD_ARM_VEC256_WITH_SLEEF
--   CMAKE_PREFIX_PATH     : /root/anaconda3/envs/vllm/lib/python3.11/site-packages;/root/anaconda3/envs/vllm:;/usr/local/cuda;/usr/local/cuda;/usr/local/cuda;/usr/local/cuda
--   CMAKE_INSTALL_PREFIX  : /home/pytorch/torch
--   USE_GOLD_LINKER       : OFF
--
--   TORCH_VERSION         : 2.6.0
--   BUILD_STATIC_RUNTIME_BENCHMARK: OFF
--   BUILD_BINARY          : OFF
--   BUILD_CUSTOM_PROTOBUF : ON
--     Link local protobuf : ON
--   BUILD_PYTHON          : True
--     Python version      : 3.11.11
--     Python executable   : /root/anaconda3/envs/vllm/bin/python
--     Python library      :
--     Python includes     : /root/anaconda3/envs/vllm/include/python3.11
--     Python site-package : /root/anaconda3/envs/vllm/lib/python3.11/site-packages
--   BUILD_SHARED_LIBS     : ON
--   CAFFE2_USE_MSVC_STATIC_RUNTIME     : OFF
--   BUILD_TEST            : True
--   BUILD_JNI             : OFF
--   BUILD_MOBILE_AUTOGRAD : OFF
--   BUILD_LITE_INTERPRETER: OFF
--   INTERN_BUILD_MOBILE   :
--   TRACING_BASED         : OFF
--   USE_BLAS              : 1
--     BLAS                : open
--     BLAS_HAS_SBGEMM     :
--   USE_LAPACK            : 1
--     LAPACK              : open
--   USE_ASAN              : OFF
--   USE_TSAN              : OFF
--   USE_CPP_CODE_COVERAGE : OFF
--   USE_CUDA              : 1
--     Split CUDA          :
--     CUDA static link    : OFF
--     USE_CUDNN           : ON
--     USE_CUSPARSELT      : OFF
--     USE_CUDSS           : OFF
--     USE_CUFILE          : OFF
--     CUDA version        : 12.6
--     USE_FLASH_ATTENTION : ON
--     USE_MEM_EFF_ATTENTION : ON
--     cuDNN version       : 8.9.7
--     CUDA root directory : /usr/local/cuda
--     CUDA library        : /usr/lib64/libcuda.so
--     cudart library      : /usr/local/cuda/lib64/libcudart.so
--     cublas library      : /usr/local/cuda/lib64/libcublas.so
--     cufft library       : /usr/local/cuda/lib64/libcufft.so
--     curand library      : /usr/local/cuda/lib64/libcurand.so
--     cusparse library    : /usr/local/cuda/lib64/libcusparse.so
--     cuDNN library       : /usr/local/cuda/lib64/libcudnn.so
--     nvrtc               : /usr/local/cuda/lib64/libnvrtc.so
--     CUDA include path   : /usr/local/cuda/include
--     NVCC executable     : /usr/local/cuda/bin/nvcc
--     CUDA compiler       : /usr/local/cuda/bin/nvcc
--     CUDA flags          :  -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__
--     CUDA host compiler  :
--     CUDA --device-c     : OFF
--     USE_TENSORRT        :
--   USE_XPU               : OFF
--   USE_ROCM              : OFF
--   BUILD_NVFUSER         :
--   USE_EIGEN_FOR_BLAS    :
--   USE_FBGEMM            : OFF
--     USE_FAKELOWP          : OFF
--   USE_KINETO            : ON
--   USE_GFLAGS            : OFF
--   USE_GLOG              : OFF
--   USE_LITE_PROTO        : OFF
--   USE_PYTORCH_METAL     : OFF
--   USE_PYTORCH_METAL_EXPORT     : OFF
--   USE_MPS               : OFF
--   CAN_COMPILE_METAL     :
--   USE_MKL               :
--   USE_MKLDNN            : OFF
--   USE_UCC               : OFF
--   USE_ITT               : OFF
--   USE_NCCL              : 1
--     USE_SYSTEM_NCCL     : ON
--   USE_NNPACK            : ON
--   USE_NUMPY             : ON
--   USE_OBSERVERS         : ON
--   USE_OPENCL            : OFF
--   USE_OPENMP            : ON
--   USE_MIMALLOC          : OFF
--   USE_VULKAN            : OFF
--   USE_PROF              : OFF
--   USE_PYTORCH_QNNPACK   : ON
--   USE_XNNPACK           : ON
--   USE_DISTRIBUTED       : 1
--     USE_MPI               : OFF
--     USE_GLOO              : ON
--     USE_GLOO_WITH_OPENSSL : OFF
--     USE_TENSORPIPE        : ON
--   Public Dependencies  :
--   Private Dependencies : Threads::Threads;/usr/lib64/libopenblas.so;pthreadpool;cpuinfo;pytorch_qnnpack;nnpack;XNNPACK;microkernels-prod;fp16;caffe2::openmp;tensorpipe;nlohmann;gloo;rt;fmt::fmt-header-only;kineto;gcc_s;gcc;dl
--   Public CUDA Deps.    :
--   Private CUDA Deps.   : caffe2::curand;caffe2::cufft;caffe2::cublas;torch::cudnn;__caffe2_nccl;tensorpipe_cuda;gloo_cuda;fmt::fmt-header-only;/usr/local/cuda/lib64/libcudart.so;CUDA::cusparse;CUDA::cufft;ATEN_CUDA_FILES_GEN_LIB
--   USE_COREML_DELEGATE     : OFF
--   BUILD_LAZY_TS_BACKEND   : ON
--   USE_ROCM_KERNEL_ASSERT : OFF
-- Performing Test HAS_WMISSING_PROTOTYPES
-- Performing Test HAS_WMISSING_PROTOTYPES - Failed
-- Performing Test HAS_WERROR_MISSING_PROTOTYPES
-- Performing Test HAS_WERROR_MISSING_PROTOTYPES - Failed
-- Configuring done (59.7s)
-- Generating done (2.5s)

'''


7、安装vllm 0.8.1（会占用很多CPU和内存 free -h）
    有时候git clone thirdparty会超时：https://www.cnblogs.com/balabalabubalabala/p/17065553.html

    yum install kmod

    git clone https://github.com/vllm-project/vllm.git
    cd vllm
    python use_existing_torch.py
    ###新版不需要执行：pip --default-timeout=1000 install -r requirements-build.txt -i https://mirrors.huaweicloud.com/repository/pypi/simple
    MAX_JOBS=3 pip --default-timeout=1000 install -e . --no-build-isolation -i https://mirrors.huaweicloud.com/repository/pypi/simple
    或者 MAX_JOBS=3 pip --default-timeout=1000 install -e . --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple

    后台编译：
    export MAX_JOBS=3
    nohup pip --default-timeout=1000 install -e . --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple > build.log 2>&1 &

    MAX_JOBS=4大概需要12~16G内存
    #VLLM_TARGET_DEVICE=cuda（不配置默认GPU版本）
    #VLLM_TARGET_DEVICE=cpu
    有时候安装会失败，由于git clone下载依赖repo失败，多试几次或者翻墙

    1、Failed to compute shorthash for libnvrtc.so
       ====》https://github.com/pytorch/pytorch/issues/53350
       ====》貌似可以忽略
    2、nvcc error   : '"$CICC_PATH/cicc"' died due to signal 9 (Kill signal)
       ====》MAX_JOBS太大，导致内存不足，改成MAX_JOBS=2或者放大内存
    3、 /usr/bin/ld: final link failed: No space left on device
        collect2: error: ld returned 1 exit status
        ninja: build stopped: subcommand failed.
       ====》磁盘空间不足
    4、/bin/sh: line 1: lsmod: command not found