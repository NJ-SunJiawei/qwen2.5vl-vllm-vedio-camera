拉取openeuler容器环境：docker pull openeuler-22.03-lts:latest
容器环境需要安装nvidia-container-toolkit
docker run -itd -e TMOUT=0 --gpus=all  --name conda-env -v /home:/home -p 8000:8000 openeuler-22.03-lts
docker exec -e TMOUT=0 -it conda-env bash

docker run -itd -e TMOUT=0 --gpus=all  --name qwenvl-env -v /home:/home -p 8000:8000 openeuler-22.03-lts:vllm-ok
docker exec -e TMOUT=0 -it qwenvl-env bash
conda activate torch

vllm退出后显存被占用
ps -ef |grep nvidia
nvidia-smi pmon
fuser -v /dev/nvidia*
或者清理容器


NVIDIA和CUDA 版本的选择，以torch和vllm能支持的版本为主选择，目前选择12.40+550.54.14 ✔✔✔
(宿主机CUDA可以不安装，然后安装在容器里，注意要和torch的cuda版本一致✔✔✔)
(宿主机NVIDIA驱动貌似可以低点版本 容器CUDA12.40+宿主DRIVER535.129.03✔✔✔)

1、安装nvidia driver和CUDA(宿主机安装、注意pyorch和vllm的安装不需要CUDA，只有源码编译时候才需要CUDA)
    cuda和驱动版本对照表：https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html
    腾讯云环境：NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0
                NVIDIA-SMI 535.129.03   Driver Version: 535.129.03   CUDA Version: 12.2 
    https://my.oschina.net/openeuler/blog/17531185

    # 安装必要依赖
    sudo dnf install -y kernel-devel kernel-headers gcc make cmake git python3-devel tar zip
    sudo dnf groupinstall -y "Development Tools"

2.1 安装 NVIDIA 驱动（适配 openEuler 24.03）
    # 检查内核版本（需与驱动兼容）
    uname -r  # 例如：5.15.0-101.oe2403.x86_64

    # 禁用nouveau驱动
    echo "blacklist nouveau" | sudo tee /etc/modprobe.d/blacklist-nouveau.conf
    sudo dracut --force

    # 下载并安装驱动（选择CUDA 12.2+兼容版本）
    https://www.nvidia.com/en-us/drivers/
    wget https://us.download.nvidia.com/tesla/535.129.03/NVIDIA-Linux-x86_64-535.129.03.run
    wget https://us.download.nvidia.com/tesla/550.54.14/NVIDIA-Linux-x86_64-550.54.14.run
    sudo sh NVIDIA-Linux-x86_64-535.129.03.run --silent --dkms

    # 验证驱动
    nvidia-smi

    #驱动卸载 https://cloud.tencent.com/document/product/560/111921
    nvidia-installer --uninstall -s
    lsmod | grep nvidia 检查驱动是否卸载干净
    fuser -k /dev/nvidia*; rmmod nvidia_modeset; rmmod nvidia_drm; rmmod nvidia_uvm; rmmod nvidia;

2.2 安装 CUDA 12.2 + cuDNN 8.9 + NCCL 2.18
    (宿主机可以不安装，然后安装在容器里，注意要和torch的cuda版本一致✔✔✔)
    # 安装CUDA Toolkit
    wget https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_535.104.05_linux.run
    sudo sh cuda_12.2.2_535.104.05_linux.run --silent --toolkit

    # 安装cuDNN和NCCL（需从NVIDIA官网下载） #http://developer.download.nvidia.com/compute/redist/cudnn/
                                            #https://developer.nvidia.com/rdp/cudnn-archive
    tar -xzf cudnn-linux-x86_64-8.9.6.50_cuda12-archive.tar.xz
    sudo cp -r cudnn-*-archive/include/* /usr/local/cuda/include/
    sudo cp -r cudnn-*-archive/lib/* /usr/local/cuda/lib64/

    tar -xzf nccl_2.18.5-1+cuda12.2_x86_64.txz
    sudo cp -r nccl_*/lib/* /usr/local/cuda/lib64/
    sudo cp nccl_*/include/* /usr/local/cuda/include/

    # 添加环境变量
    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
    echo 'export CUDA_HOME=/usr/local/cuda:$CUDA_HOME' >> ~/.bashrc
    source ~/.bashrc

    部分容器环境需要使用下面：✔✔✔
    echo 'export PATH=/usr/local/cuda/bin' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64' >> ~/.bashrc
    echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
    source ~/.bashrc

    #https://developer.nvidia.com/cuda-toolkit-archive   cuda-toolkit安装包
 

3、安装Conda环境
   Linux 用户，可以下载 Anaconda3-2022.05-Linux-x86_64.sh
   wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
   # 先进行 conda 的安装:
   bash Anaconda3-2022.05-Linux-x86_64.sh
   source ~/.bashrc
   conda --version

   #让 conda shell 环境常驻(不常使用不推荐):
   eval "$(~/anaconda3/bin/conda shell.bash hook)"

   #国内用户，推荐在使用 Conda 时，先进行软件源配置操作：
   vi ~/.condarc
   '''
channels:
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
 - defaults
show_channel_urls: true
   '''
   完成了 ~/.condarc 的内容修改之后，先重启 Shell，接着使用 conda info 就可以检查软件源是否配置成功了
   conda info

   #手动激活 conda shell
   conda create -n torch python=3.11 -y
   
   #激活和退出专用环境：
   conda activate torch
   conda deactivate
   conda list                  当前env基础环境
   conda list -n torch
   conda env list              env列表
   conda activate base
   conda env remove -n torch   删除env
   conda remove -n torch --all 
   conda install --use-local xxxx.tar.bz2(xxxx.tar.bz2是包的绝对路径) 安装离线py包
   
   #当完成环境激活之后，我们同样先来完成pip软件源的切换
   pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

3  安装VLLM或者PyTorch

3.1  安装PyTorch  (方法1，在线安装)
           pip install torch torchvision torchaudio（cpu版本）
           pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 （x86_64 GPU版本）✔✔✔
           pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124 （x86_64 GPU版本）（指定版本，发现没有arm版本的GPU，只有arm版本的CPU）
           pip list
           '''
            torch                    2.6.0+cu124
            torchaudio               2.6.0+cu124
            torchvision              0.21.0+cu124
           '''
           python -c "import torch; print(torch.cuda.device_count())"
           python -c "import torch; [print(f'CUDA device {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())] if torch.cuda.device_count() > 0 else print('Using CPU')"

3.2 安装PyTorch  (方法2，离线安装)✔✔✔
        #版本选择：https://download.pytorch.org/whl/cu124/torch/
                   https://download.pytorch.org/whl/nightly/cu124
        arm64: wget https://download.pytorch.org/whl/nightly/cu126/torch-2.6.0.dev20250104%2Bcu126-cp39-cp39-linux_aarch64.whl
               pip install torch-2.6.0.dev20250104+cu126-cp39-cp39-linux_aarch64.whl -i https://mirrors.huaweicloud.com/repository/pypi/simple

        x86_64: wget https://download.pytorch.org/whl/cu124_full/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl#sha256=0851a56527529b135e1f44e6b5826effb8f6a19368d9eaba9104d94a7b21affc
                pip install torch-2.6.0+cu124-cp311-cp311-linux_x86_64.whl -i https://mirrors.huaweicloud.com/repository/pypi/simple
                
        依赖：pip install numpy -i https://mirrors.huaweicloud.com/repository/pypi/simple 
        cp311代表python3.11版本✔✔✔
        
        # 验证GPU支持
        python -c "import torch; print(torch.cuda.device_count())"
        python -c "import torch; print(torch.__version__); print(torch.__path__)"
        python -c "import torch; [print(f'CUDA device {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())] if torch.cuda.device_count() > 0 else print('Using CPU')"

     vLLM  need python>3.8 ✔✔✔
4.1  安装vLLM+PyTorch （方法1 Qwen2.5-VL)
    yum groupinstall "Development Tools"
    在线安装：pip install vllm -i https://mirrors.huaweicloud.com/repository/pypi/simple ✔✔✔ 版本0.7.3+torch2.5.1+cu124
              pip show vllm
             首先明确需要的torch对应版本（注：如果pip install vllm，会自动安装一个torch2.5.1+cu124的版本，不匹配cuda版本，运行报错）

         (暂时不推荐安装)pip install flash-attn --no-build-isolation -i https://mirrors.huaweicloud.com/repository/pypi/simple ✔✔✔（如果你的显卡支持fp16或bf16精度，我们还推荐安装flash-attention（当前已支持flash attention 2）来提高你的运行效率以及降低显存占用。(flash-attention只是可选项，不安装也可正常运行该项目)）
         报错：OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.===》容器内安装CUDA
               https://developer.nvidia.com/cuda-toolkit-archive
               wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run
               sudo sh cuda_12.4.0_550.54.14_linux.run
               安装CUDA版本不一致运行torch会报错：ImportError: /root/anaconda3/envs/torch/lib/python3.11/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12
                                 cuda的__nvJitLinkComplete_12_4版本和torch匹配不上
                                 find / -name libnvJitLink.so.12 ==》可能查询出多个，按照env链接对应的
                                 ln -s /root/anaconda3/envs/torch/lib/python3.11/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12 \
                                       /root/anaconda3/envs/torch/lib/python3.11/site-packages/nvidia/cusparse/lib/libnvJitLink.so.12
                                 export LD_LIBRARY_PATH=/root/anaconda3/envs/torch/lib/python3.11/site-packages/nvidia/cusparse/lib:$LD_LIBRARY_PATH
                                 #https://blog.csdn.net/qq_42730750/article/details/139582293
        报错：FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/cuda:/bin/nvcc' ✔✔✔
              确认是否安装nvcc -V
              查找二进制路径 which nvcc  ===》/usr/local/cuda/bin/nvcc
              修改 export CUDA_HOME=/usr/local/cuda 
              #https://blog.csdn.net/weixin_45320238/article/details/144259578

       网络问题可能会卡住：？？？
       pip install /path/to/downloaded/flash_attn-2.6.1+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

      #pip install ray （安装ray， 以便支持分布式服务）
      官方分布式推理：#https://vllm-zh.llamafactory.cn/serving/distributed_serving.html

     flash-attn和torch关系
     https://blog.csdn.net/p731heminyang/article/details/141676862

4.2  安装vLLM  (方法2 DeepSeek) {***}
     vllm构建依赖安装
     如果在容器内，在安装一遍CUDA
     yum groupinstall "Development Tools"
     pip install setuptools_scm / yum install kmod / sh cuda_12.2.2_535.104.05_linux.run 
     cmake源码安装（注意版本> 3.26.0）：wget https://cmake.org/files/v3.26/cmake-3.26.6-linux-x86_64.sh
                                        wget https://github.com/Kitware/CMake/releases/download/v3.31.2/cmake-3.31.2-linux-x86_64.sh
                                        mv cmake-3.31.2-linux-x86_64 /opt/cmake-3.31.2
                                        ln -sf /opt/cmake-3.31.2/bin/*  /usr/bin

    离线安装vllm （GPU版本）：git clone https://github.com/vllm-project/vllm.git    #版本vllm==0.7.4

                              git clone --branch v3.8.0 https://github.com/nvidia/cutlass.git #版本 v3.8.0
                              或者git clone https://github.com/nvidia/cutlass.git/ 
                                  git checkout 06b21349bcf6ddf6a1686a47a137ad1446579db9 / v3.8.0
                              cp -r * /home/vllm/.deps/cutlass-src

                              git clone https://github.com/ROCm/composable_kernel.git  #版本rocm-6.3.3
                              cp -r * /home/vllm/.deps/vllm-flash-attn-src/csrc/composable_kernel
             cd vllm
             python use_existing_torch.py
             ###pip install -r requirements-build.txt -i https://mirrors.huaweicloud.com/repository/pypi/simple
             MAX_JOBS=2 pip install -e . --no-build-isolation -i https://mirrors.huaweicloud.com/repository/pypi/simple
                     ====》最后实际执行
                         cmake  \
                        -DCMAKE_BUILD_TYPE=RelWithDebInfo \
                        -DVLLM_TARGET_DEVICE=cuda \
                        -DVLLM_PYTHON_EXECUTABLE=/root/anaconda3/envs/torch-lixian/bin/python \
                        -DVLLM_PYTHON_PATH=/root/anaconda3/envs/torch-lixian/lib/python311.zip:/root/anaconda3/envs/torch-lixian/lib/python3.11:/root/anaconda3/envs/torch-lixian/lib/python3.11/lib-dynload:/root/anaconda3/envs/torch-lixian/lib/python3.11/site-packages:/root/anaconda3/envs/torch-lixian/lib/python3.11/site-packages/setuptools/_vendor \
                        -DFETCHCONTENT_BASE_DIR=/home/vllm/.deps \
                        -DNVCC_THREADS=1
             报错：1、pip install setuptools_scm --no-build-isolation  -i https://mirrors.huaweicloud.com/repository/pypi/simple
                   2、报错：AssertionError: CUDA_HOME is not set ----- 容器内安装CUDA
                                  https://developer.nvidia.com/cuda-toolkit-archive
                                  wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run
                                  sudo sh cuda_12.4.0_550.54.14_linux.run
                                 安装CUDA版本不一致运行torch会报错：ImportError: /root/anaconda3/envs/torch/lib/python3.11/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12
                                 cuda的__nvJitLinkComplete_12_4版本和torch匹配不上
                                 find / -name libnvJitLink.so.12 ==》可能查询出多个，按照env链接对应的
                                 ln -s /root/anaconda3/envs/torch/lib/python3.11/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12 \
                                       /root/anaconda3/envs/torch/lib/python3.11/site-packages/nvidia/cusparse/lib/libnvJitLink.so.12
                                 export LD_LIBRARY_PATH=/root/anaconda3/envs/torch/lib/python3.11/site-packages/nvidia/cusparse/lib:$LD_LIBRARY_PATH
                                 #https://blog.csdn.net/qq_42730750/article/details/139582293
                   3、yum install kmod
                   4、LookupError: setuptools-scm was unable to detect version for：===> 必须是git目录，有git记录
                   5、RuntimeError: Cannot find CMake executable ===>yum install cmake
                   6、Failed to detect a default CUDA architecture.===》不要忘记添加CUDA添加环境变量到PATH
                   7、MAKE_CUDA_ARCHITECTURES must be non-empty if set.===》删除旧的编译缓存
                   8、fatal: unable to access 'https://github.com/nvidia/cutlass.git/' ===》vllm/CMakeLists.txt 195行修改 GIT_REPOSITORY https://github.com/nvidia/cutlass.git=> GIT_REPOSITORY <your_local_dir_for_cutlass>
                      #https://github.com/vllm-project/vllm/issues/7368


    离线安装vllm （CPU版本）：git clone https://github.com/vllm-project/vllm.git
            cd vllm
            VLLM_TARGET_DEVICE=cpu pip install -e . -i https://mirrors.huaweicloud.com/repository/pypi/simple

5    下载大模型：
    deepseek：        
            pip install huggingface-hub -i https://mirrors.huaweicloud.com/repository/pypi/simple
            export HF_ENDPOINT="https://hf-mirror.com"
            huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Llama-8B --local-dir DeepSeek-R1-Distill-Llama-8B
            启动vllm server：
            python -m vllm.entrypoints.openai.api_server \
                --served-model-name deepseek-r1:8b \
                --model /home/deepseek/DeepSeek-R1-Distill-Llama-8B \
                --trust-remote-code \
                --host 0.0.0.0 \
                --port 8080 \
                --max-model-len 4096 \
                --tensor-parallel-size 1 \
                --gpu_memory_utilization 0.8 \
                --enforce-eager \
                --dtype float16
            #--tensor-parallel-size 指定张量并行的数量，设置为 8 表示模型将会在 8 个 GPU 上进行并行计算，读者需要根据自己机器的实际 GPU 数量填写。
            
            请求数据:
            curl "http://localhost:8080/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "deepseek-r1",
              "messages": [{"role": "user", "content": "请介绍一下开源操作系统openEuler"}],
            }'

            curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{
                "model": "gpt-4",
                "messages": [
                    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
                    {"role": "user", "content": "Tell me something about large language models."}
                ],
                "temperature": 0.7,
                "top_p": 0.8,
                "repetition_penalty": 1.05,
                "max_tokens": 512
            }'
            
    Qwen2.5-VL：✔✔✔
           apt-get install build-essential           
           
           #官网：https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html#    
            
            pip install "huggingface_hub[hf_transfer]" -i https://mirrors.huaweicloud.com/repository/pypi/simple   #会安装huggingface-hub和高效文件传输 (hf_transfer)
            export HF_ENDPOINT="https://hf-mirror.com"
            HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Qwen/Qwen2.5-VL-3B-Instruct
            
            添加到环境变量：vim ~/.bashrc 或者 vim /etc/profile 或者 /etc/bashrc
            
            启动vllm server：
            使用在线下载模型：✔✔✔
#            VLLM_USE_V1=1 \
            export HF_HUB_OFFLINE=1
            VLLM_WORKER_MULTIPROC_METHOD=spawn \
            vllm serve Qwen/Qwen2.5-VL-3B-Instruct --trust-remote-code --served-model-name Qwen/Qwen2.5-VL-3B-Instruct --gpu-memory-utilization 0.8 --max-model-len 4096 --tensor-parallel-size 1 --port 8000 --dtype=float16 --max_num_batched_tokens=2048
            ####--dtype=float16等于--dtype=half  T4不支持 --dtype=auto
            
            使用本地离线模型：✔✔✔
#            VLLM_USE_V1=1 \
            VLLM_WORKER_MULTIPROC_METHOD=spawn \
            vllm serve /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/1b989f2c63999d7344135894d3cfa8f494116743 --trust-remote-code --served-model-name Qwen/Qwen2.5-VL-3B-Instruct --gpu-memory-utilization 0.8 --max-model-len 8192 --tensor-parallel-size 1 --host 0.0.0.0 --port 8000 --dtype=float16

            export HF_HUB_OFFLINE=1
            VLLM_WORKER_MULTIPROC_METHOD=spawn vllm serve Qwen/Qwen2.5-VL-3B-Instruct --trust-remote-code --served-model-name Qwen/Qwen2.5-VL-3B-Instruct --gpu-memory-utilization 0.8 --max-model-len 8192 --tensor-parallel-size 1 --host 0.0.0.0 --port 8000 --dtype=float16
           
            ###--max_num_batched_tokens=2048
            ###--limit-mm-per-prompt image=5,video=5
            ####官方配置：https://github.com/QwenLM/Qwen2.5-VL
#                         vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5
#                            curl http://localhost:8000/v1/chat/completions \
#                            -H "Content-Type: application/json" \
#                            -d '{
#                            "model": "Qwen/Qwen2.5-VL-7B-Instruct",
#                            "messages": [
#                            {"role": "system", "content": "You are a helpful assistant."},
#                            {"role": "user", "content": [
#                                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
#                                {"type": "text", "text": "What is the text in the illustrate?"}
#                            ]}
#                            ]
#                            }'

            使用本地离线模型（OpenAI Completions API）：✔✔✔
            python -m vllm.entrypoints.openai.api_server --model /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/1b989f2c63999d7344135894d3cfa8f494116743 --trust-remote-code --served-model-name Qwen/Qwen2.5-VL-3B-Instruct --gpu-memory-utilization 0.8 --max-model-len 4096 --tensor-parallel-size 1 --host 0.0.0.0 --port 8000 --dtype=float16
            #--limit-mm-per-prompt image=5,video=5
            #https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md
                使用脚本测试✔✔✔
                curl http://localhost:8000/v1/chat/completions \
                -H "Content-Type: application/json" \
                -d '{
                    "model": "Qwen/Qwen2.5-VL-3B-Instruct",
                    "messages":[
                      {"role": "system", "content": "你是一个有用的助手。"},
                      {"role": "user", "content": [
                        {"type": "image_url", 
                         "image_url": {
                           "url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}
                        },
                        {"type": "text", "text": "插图中的文本是什么？"}
                      ]
                      }
                    ]
                }'


            #--trust-remote-code: 允许执行远程代码。
            #--served-model-name gpt-4: 将服务器提供的模型名称设置为gpt-4。
            #--gpu-memory-utilization 0.98: 设置GPU内存利用率为98%。
            #--tensor-parallel-size 4: 设置张量并行处理的大小为4。
            #--host 0.0.0.0
            #--port 8000: 在端口8000上启动服务器。
            #--api-keys 12369874
            #--max_num_seqs 256 默认256
            #https://blog.csdn.net/qq_41527980/article/details/139856790 参数表
            #https://developer.volcengine.com/articles/7396884793574555711 重要参数


            查询模型：curl http://localhost:8000/v1/models

            关闭vllm：
            pgrep -f vllm
            ps aux | grep vllm
            kill -9 ***

             Processes:                                                                            |
            |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
            |        ID   ID                                                             Usage      |
            |=======================================================================================|
            |    0   N/A  N/A   1782901      C   /root/anaconda3/envs/torch/bin/python     14568MiB
            kill -9 PID

            报错：
            Error retrieving file list: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/Qwen/Qwen2.5-VL-3B-Instruct/tree/main?recursive=True&expand=False (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f17a53dbcd0>, 'Connection to huggingface.co timed out. (connect timeout=None)'))")
            解决：export HF_ENDPOINT="https://hf-mirror.com"

    python -c "import torch; from vllm import LLM; print(f'PyTorch CUDA available: {torch.cuda.is_available()}'); model = LLM('Qwen/Qwen2.5-VL-3B-Instruct', tensor_parallel_size=1); device = 'cuda' if torch.cuda.is_available() else 'cpu'; model.to(device); print(f'Model device: {next(model.parameters()).device if torch.cuda.is_available() else \"cpu\"}')"            

6   deepseek VLLM容器环境：
    docker pull hub.oepkgs.net/neocopilot/deepseek_vllm:openeEuler2203-lts-sp4_gpu
    docker run --gpus all --name deepseek_kunpeng_gpu -it hub.oepkgs.net/neocopilot/deepseek_vllm:openeEuler2203-lts-sp4_gpu bash
    vllm serve /home/deepseek/model/DeepSeek-R1-Distill-Qwen-7B/ --tensor-parallel-size 8 --max_model_len 32768 &
    #https://my.oschina.net/openeuler/blog/17646860

7   dify安装 ✔✔✔
    git clone https://github.com/langgenius/dify.git
    cd dify/docker
    docker-compose up -d

    docker源地址：
    '''
        "https://docker.registry.cyou",
        "https://docker-cf.registry.cyou",
        "https://dockercf.jsdelivr.fyi",
        "https://docker.jsdelivr.fyi",
        "https://dockertest.jsdelivr.fyi",
        "https://mirror.aliyuncs.com",
        "https://dockerproxy.com",
        "https://mirror.baidubce.com",
        "https://docker.m.daocloud.io",
        "https://docker.nju.edu.cn",
        "https://docker.mirrors.sjtug.sjtu.edu.cn",
        "https://docker.mirrors.ustc.edu.cn",
        "https://mirror.iscas.ac.cn",
        "https://docker.rainbond.cc"
    '''
    #https://east.moe/archives/1478
    dify如何配置vllm==>设置 --> 模型供应商 --> OpenAI-API-compatible --> 编辑 API 

    #https://news.qq.com/rain/a/20250304A057HO00?suid=&media_id=
    #https://www.cnblogs.com/eslzzyl/p/18378226
    OpenAI的API Key: EMPTY #vLLM 服务不需要 API 密钥，可以使用任意字符串
    API Base: http://localhost:10086/v1

#q：https://blog.csdn.net/m0_65814643/article/details/144110567

参考：部署：https://blog.csdn.net/engchina/article/details/145455322
            https://zhuanlan.zhihu.com/p/23790722387
            https://zhuanlan.zhihu.com/p/23533488927
            qwen2-vl部署：https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md
      qwen2.5-vl UI：https://blog.csdn.net/weixin_42684822/article/details/145515609?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ECtr-2-145515609-blog-145455322.235%5Ev43%5Epc_blog_bottom_relevance_base7&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ECtr-2-145515609-blog-145455322.235%5Ev43%5Epc_blog_bottom_relevance_base7&utm_relevant_index=5
      github: https://github.com/QwenLM/Qwen2.5-VL